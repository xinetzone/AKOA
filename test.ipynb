{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "class PathTF:\n",
    "    def __init__(self, path_list):\n",
    "        self.path_ds = tf.data.Dataset.from_tensor_slices(path_list)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.path_ds)\n",
    "    \n",
    "    def split_iter(self):\n",
    "        n = len(self)\n",
    "        batch_size = int(tf.math.ceil(n/3))\n",
    "        # 打乱\n",
    "        path_ds = self.path_ds.shuffle(n)\n",
    "        return path_ds.batch(batch_size)\n",
    "\n",
    "def preprocess_image(image, width=224, height=224):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [width, height])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, left_paths, right_paths):\n",
    "        left = PathTF(left_paths)\n",
    "        right = PathTF(right_paths)\n",
    "        self.left_path_ds = [ds for ds in left.split_iter()]\n",
    "        self.right_path_ds = [ds for ds in right.split_iter()]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        left_xs = self.left_path_ds[index]\n",
    "        left_ys = tf.zeros(len(left_xs), dtype=tf.int64)\n",
    "        right_xs = self.right_path_ds[index]\n",
    "        right_ys = tf.ones(len(right_xs), dtype=tf.int64)\n",
    "        xs = tf.concat((left_xs, right_xs), axis=0)\n",
    "        ys = tf.concat((left_ys, right_ys), axis=0)\n",
    "        paths = tf.data.Dataset.from_tensor_slices(xs)\n",
    "        image_ds = paths.map(load_and_preprocess_image)\n",
    "        label_ds = tf.data.Dataset.from_tensor_slices(ys)\n",
    "        image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "        return image_label_ds\n",
    "    \n",
    "    def finish(self, image_label_ds, batch_size):\n",
    "        # 设置一个和数据集大小一致的 shuffle buffer size（随机缓冲区大小）以保证数据\n",
    "        # 被充分打乱。\n",
    "        n = len(image_label_ds)\n",
    "        ds = image_label_ds.shuffle(buffer_size=n)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.batch(batch_size)\n",
    "        # 当模型在训练的时候，`prefetch` 使数据集在后台取得 batch。\n",
    "        ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'data/'\n",
    "batch_size = 32\n",
    "\n",
    "data_root = Path(data_root)\n",
    "left_paths = [path.as_posix() for path in data_root.rglob('**/left/**/*png')]\n",
    "right_paths =[path.as_posix() for path in data_root.rglob('**/right/**/*png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Dataset(left_paths, right_paths)\n",
    "\n",
    "train, val, test = loader[0], loader[1], loader[2]\n",
    "trainset = loader.finish(train, batch_size)\n",
    "\n",
    "valset = loader.finish(val, batch_size)\n",
    "testset = loader.finish(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 1]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[1 0 1 ... 0 1 0]\n",
      "[1 0 1 ... 0 1 1]\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 0 0 1]\n",
      "[1 0 0 ... 0 0 0]\n",
      "[1 0 0 ... 0 1 0]\n",
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "for k, batch in enumerate(trainset):\n",
    "    tf.print(batch[1])\n",
    "    if k > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102875136/102869336 [==============================] - 18s 0us/step\n"
     ]
    }
   ],
   "source": [
    "ResNet50 = tf.keras.applications.resnet_v2.ResNet50V2(weights='imagenet', input_shape=(224,224,3))\n",
    "for layer in ResNet50.layers:\n",
    "    layer.trainable = False\n",
    "net = tf.keras.models.Sequential()\n",
    "net.add(ResNet50)\n",
    "net.add(tf.keras.layers.Flatten())\n",
    "net.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50v2 (Functional)      (None, 1000)              25613800  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 25,615,802\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 25,613,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in tf.range(EPOCHS):\n",
    "  # 在下一个epoch开始时，重置评估指标\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for images, labels in trainset:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in valset:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(),\n",
    "                         test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoAI",
   "language": "python",
   "name": "autoai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
